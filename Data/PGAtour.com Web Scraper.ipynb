{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PGAtour.com Web Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to display the web scraper I created to scrape the PGAtour.com website for pga tour player statistics from 2010-2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import requests # Request module\n",
    "import pandas as pd # Data Wrangling\n",
    "import numpy as np # Data Wrangling\n",
    "from bs4 import BeautifulSoup #Web sraping module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PGA tour statistical data is contained on separate pages on pgatour.com/stats webste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My approach to scraping this data from these separate web pages was to \n",
    "<ol>\n",
    "<li>Create a dataframe for each statistic page. Each dataframe includes the players and their stats.</li>\n",
    "<li>Keep only the columns that I need from that page.</li>\n",
    "<li>Repeat steps 1-3 for years 2010-2017.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of my strategy is described below\n",
    "<ol>\n",
    "<li>Pulls column headers</li>\n",
    "<li>Pulls players from particular stats page</li>\n",
    "<li>Pulls statistics from page</li>\n",
    "<li>Create a dictionary to store player data in for particular stats page.</li>\n",
    "<li>Uses functions 1-4 to create a pandas dataframe to store data for that particular statistic.</li>\n",
    "<li>Loop through years 2010-2017 to create a dataframe from years 2010-2017</li>\n",
    "<li>Store results in a sqlite3 database for future use.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pull column headers from page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_headers(soup):\n",
    "    '''This function get's the column names to use for the data frame.'''\n",
    "    headers = []\n",
    "    \n",
    "    #Get rounds header\n",
    "    rounds = soup.find_all(class_=\"rounds hidden-small hidden-medium\")[0].get_text()\n",
    "    headers.append(rounds)\n",
    "    \n",
    "    #Get other headers\n",
    "    stat_headers = soup.find_all(class_=\"col-stat hidden-small hidden-medium\")\n",
    "    for header in stat_headers:\n",
    "        headers.append(header.get_text())\n",
    "    \n",
    "    return headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pull players from page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get Players\n",
    "def get_players(soup):\n",
    "    '''This function takes the beautiful soup created and uses it to gather player names from the specified stats page.'''\n",
    "    \n",
    "    player_list = []\n",
    "    \n",
    "    #Get player as html tags\n",
    "    players = soup.select('td a')[1:] #Use 1 beacuse first line of all tables is not useful.\n",
    "    #Loop through list\n",
    "    for player in players:\n",
    "        player_list.append(player.get_text())\n",
    "    \n",
    "    return player_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Pull statistics from page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Get Stats\n",
    "def get_stats(soup, categories):\n",
    "    '''This function takes the soup created before and the number of categories needed to generate this'''\n",
    "    \n",
    "    #Finds all tags with class specified and puts into a list\n",
    "    stats = soup.find_all(class_=\"hidden-small hidden-medium\")\n",
    "    \n",
    "    #Initialize stats list\n",
    "    stat_list = []\n",
    "    \n",
    "    #Loop through \n",
    "    for i in range(0, len(stats)-categories+1, categories):\n",
    "        temp_list = []\n",
    "        for j in range(categories):\n",
    "            temp_list.append(stats[i + j].get_text())\n",
    "        stat_list.append(temp_list)\n",
    "            \n",
    "    return stat_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create data dictionary for page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stats_dict(players, stats):\n",
    "        '''This function takes two lists, players and stats, \n",
    "        and creates a dictionary with the player being the key \n",
    "        and the stats as the vales (as a list)'''\n",
    "    \n",
    "        #initialize player dictionary\n",
    "        player_dict = {}\n",
    "    \n",
    "        #Loop through player list\n",
    "        for i, player in enumerate(players):\n",
    "            player_dict[player] = stats[i]\n",
    "    \n",
    "        return player_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Use functions 1-4 to create dataframe for statistic. \"make_dataframe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Mega function\n",
    "def make_dataframe(url, categories):\n",
    "        \n",
    "    ##Create soup object from url.\n",
    "    response = requests.get(url)\n",
    "    text = response.text\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    \n",
    "    #1. Get Headers\n",
    "    headers = get_headers(soup)\n",
    "    \n",
    "    #2. Get Players\n",
    "    players = get_players(soup)\n",
    "    \n",
    "    #3. Get Stats\n",
    "    stats = get_stats(soup, categories)\n",
    "    \n",
    "    #4. Make stats dictionary.\n",
    "    stats_dictionary = stats_dict(players, stats)\n",
    "    \n",
    "    #Make dataframe\n",
    "    frame = pd.DataFrame(stats_dictionary, index = headers).T\n",
    "    \n",
    "    #Reset index\n",
    "    frame = frame.reset_index()\n",
    "    \n",
    "    #For each Dataframe, change index column to 'NAME'\n",
    "    frame = frame.rename(index = str, columns = {'index': 'NAME'})\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Loop through years 2010-2017 to create a dataframe from years 2010-2017\n",
    "All of the data cleaning and preprocessing happens in the next couple of code blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "years = [str(i) for i in range(2010, 2018)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n"
     ]
    }
   ],
   "source": [
    "for year in years:\n",
    "    print(year)\n",
    "    #Fedex cup points\n",
    "    fcp = make_dataframe(\"https://www.pgatour.com/stats/stat.02671.{}.html\".format(year), 6)[['NAME', 'POINTS']]\n",
    "    #Top 10's and wins\n",
    "    top10 = make_dataframe(\"https://www.pgatour.com/stats/stat.138.{}.html\".format(year), 5)[['NAME', 'TOP 10', '1ST']]\n",
    "\n",
    "    #Scoring statistics, keep rounds from this page as it most accurately reflects total rounds player completed in season.\n",
    "    scoring = make_dataframe(\"https://www.pgatour.com/stats/stat.120.{}.html\".format(year), 5)[['NAME', 'ROUNDS', 'AVG']]\n",
    "    scoring = scoring.rename(columns={'AVG':'SCORING'})\n",
    "\n",
    "    #Driving Distance\n",
    "    drivedistance = make_dataframe(\"https://www.pgatour.com/stats/stat.101.{}.html\".format(year), 4)[['NAME', 'AVG.']]\n",
    "    #Rename Columns\n",
    "    drivedistance = drivedistance.rename(columns = {'AVG.':'DRIVE_DISTANCE'})\n",
    "\n",
    "    #Driving Accuracy\n",
    "    driveacc = make_dataframe(\"https://www.pgatour.com/stats/stat.102.{}.html\".format(year), 4)[['NAME', '%']]\n",
    "    #Change column name from % to FWY %\n",
    "    driveacc = driveacc.rename(columns = {'%': \"FWY_%\"})\n",
    "\n",
    "    #Greens in Regulation.\n",
    "    gir = make_dataframe(\"https://www.pgatour.com/stats/stat.103.{}.html\".format(year), 5)[['NAME', '%']]\n",
    "    #Change column name from % to GIR %\n",
    "    gir = gir.rename(columns = {'%': \"GIR_%\"})\n",
    "\n",
    "    #Strokes gained putting\n",
    "    sg_putting = make_dataframe(\"https://www.pgatour.com/stats/stat.02564.{}.html\".format(year), 4)[['NAME', 'AVERAGE']]\n",
    "    #Change name of average column\n",
    "    sg_putting = sg_putting.rename(columns = {'AVERAGE': 'SG_P'})\n",
    "\n",
    "    #Strokes gained tee to green\n",
    "    sg_teetogreen = make_dataframe(\"https://www.pgatour.com/stats/stat.02674.{}.html\".format(year), 6)[['NAME', 'AVERAGE']]\n",
    "    #Change name of average column\n",
    "    sg_teetogreen = sg_teetogreen.rename(columns = {'AVERAGE' : 'SG_TTG'})\n",
    "\n",
    "    #sg total\n",
    "    sg_total = make_dataframe(\"https://www.pgatour.com/stats/stat.02675.{}.html\".format(year), 6)[['NAME', 'AVERAGE']]\n",
    "    sg_total = sg_total.rename(columns = {'AVERAGE':'SG_T'})\n",
    "    \n",
    "    #Get Dataframes into list.\n",
    "    data_frames = [drivedistance, driveacc, gir, sg_putting, sg_teetogreen, sg_total]\n",
    "    \n",
    "    #Merge all Dataframes together\n",
    "    df_one = pd.DataFrame()\n",
    "    df_one = scoring\n",
    "    for df in data_frames:\n",
    "        df_one = pd.merge(df_one, df, on='NAME')\n",
    "        \n",
    "    \n",
    "\n",
    "    #merge fex ex cup points\n",
    "    df_one = pd.merge(df_one, fcp, how='outer', on='NAME')\n",
    "    #Merge top 10's\n",
    "    df_one = pd.merge(df_one, top10, how='outer', on='NAME')\n",
    "    \n",
    "    #Only get people who's scoring average isn't null.\n",
    "    df_one = df_one.loc[df_one['SCORING'].isnull() == False]  \n",
    "    \n",
    "    #Add year column\n",
    "    df_one['Year'] = year\n",
    "    \n",
    "    #Concat dataframe to overall dataframe\n",
    "    \n",
    "    if year == '2010':\n",
    "        df_total = pd.DataFrame()\n",
    "        df_total = pd.concat([df_total, df_one], axis=0)\n",
    "    else:\n",
    "        df_total = pd.concat([df_total, df_one], axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>ROUNDS</th>\n",
       "      <th>SCORING</th>\n",
       "      <th>DRIVE_DISTANCE</th>\n",
       "      <th>FWY_%</th>\n",
       "      <th>GIR_%</th>\n",
       "      <th>SG_P</th>\n",
       "      <th>SG_TTG</th>\n",
       "      <th>SG_T</th>\n",
       "      <th>POINTS</th>\n",
       "      <th>TOP 10</th>\n",
       "      <th>1ST</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aaron Baddeley</td>\n",
       "      <td>94</td>\n",
       "      <td>70.995</td>\n",
       "      <td>298.9</td>\n",
       "      <td>56.65</td>\n",
       "      <td>64.60</td>\n",
       "      <td>.509</td>\n",
       "      <td>-.294</td>\n",
       "      <td>.208</td>\n",
       "      <td>559</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adam Scott</td>\n",
       "      <td>70</td>\n",
       "      <td>70.468</td>\n",
       "      <td>294.4</td>\n",
       "      <td>62.93</td>\n",
       "      <td>69.61</td>\n",
       "      <td>-.746</td>\n",
       "      <td>1.609</td>\n",
       "      <td>.862</td>\n",
       "      <td>640</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alex Cejka</td>\n",
       "      <td>81</td>\n",
       "      <td>71.219</td>\n",
       "      <td>277.4</td>\n",
       "      <td>70.31</td>\n",
       "      <td>66.60</td>\n",
       "      <td>-.466</td>\n",
       "      <td>.396</td>\n",
       "      <td>-.073</td>\n",
       "      <td>489</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alex Prugh</td>\n",
       "      <td>88</td>\n",
       "      <td>70.878</td>\n",
       "      <td>295.7</td>\n",
       "      <td>58.40</td>\n",
       "      <td>68.60</td>\n",
       "      <td>.202</td>\n",
       "      <td>-.112</td>\n",
       "      <td>.092</td>\n",
       "      <td>526</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Andres Romero</td>\n",
       "      <td>73</td>\n",
       "      <td>70.986</td>\n",
       "      <td>296.0</td>\n",
       "      <td>55.05</td>\n",
       "      <td>65.07</td>\n",
       "      <td>.254</td>\n",
       "      <td>-.118</td>\n",
       "      <td>.136</td>\n",
       "      <td>853</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             NAME ROUNDS SCORING DRIVE_DISTANCE  FWY_%  GIR_%   SG_P SG_TTG  \\\n",
       "0  Aaron Baddeley     94  70.995          298.9  56.65  64.60   .509  -.294   \n",
       "1      Adam Scott     70  70.468          294.4  62.93  69.61  -.746  1.609   \n",
       "2      Alex Cejka     81  71.219          277.4  70.31  66.60  -.466   .396   \n",
       "3      Alex Prugh     88  70.878          295.7  58.40  68.60   .202  -.112   \n",
       "4   Andres Romero     73  70.986          296.0  55.05  65.07   .254  -.118   \n",
       "\n",
       "    SG_T POINTS TOP 10 1ST  Year  \n",
       "0   .208    559      2      2010  \n",
       "1   .862    640      4   1  2010  \n",
       "2  -.073    489      4      2010  \n",
       "3   .092    526      4      2010  \n",
       "4   .136    853      2      2010  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now save the file in a sqlite3 database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load sqlite package\n",
    "import sqlite3 as db\n",
    "#Create connect object with example db. A new file will be created.\n",
    "conn = db.connect('pgatour_raw.db')\n",
    "\n",
    "#Create cursor to perform actions on db.\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daron\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:1362: UserWarning: The spaces in these column names will not be changed. In pandas versions < 0.14, spaces were converted to underscores.\n",
      "  chunksize=chunksize, dtype=dtype)\n"
     ]
    }
   ],
   "source": [
    "df_total.to_sql(\"pgatour_stats_raw\", conn, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "This notebook walked you through how I implemented a web scraper to scrape the pgatour.com website for player statistics from 2010-2017. Additional data cleaning steps will be needed to prepare this data for analysis but that is out of scope for this notebook. Applications of data cleaning in python can be seen in follow up notebooks in this repository."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
